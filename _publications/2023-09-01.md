---
title: "FedDD: Toward Communication-efficient Federated Learning with Differential Parameter Dropout2"
collection: publications
permalink: "/publication/2023-09-01"
excerpt: "Federated Learning (FL) requires frequent exchange of model parameters, which leads to long communication delay, especially when the network environments of clients vary greatly. Moreover, the parameter server needs to wait for the slowest client (i.e., straggler, which may have the largest model size, lowest computing capability or worst network condition) to upload parameters, which may significantly degrade the communication efficiency. Commonly-used client selection methods such as partial client selection would lead to the waste of computing resources and weaken the generalization of the global model. To tackle this problem, along a different line, in this paper, we advocate the approach of model parameter dropout instead of client selection, and accordingly propose a novel framework of Federated learning scheme with Differential parameter Dropout (FedDD). FedDD consists of two key modules …"
date: "2023-09-01"
venue: "IEEE Transactions on Mobile Computing, 2023"
paperurl: "https://arxiv.org/pdf/2308.16835"
author: "Z Feng, X Chen, Q Wu, W Wu, X Zhang, Q HuangIEEE Transactions on Mobile Computing, 2023"
poster:
remark:
---